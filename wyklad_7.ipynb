{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40532473",
   "metadata": {},
   "source": [
    "# Automatyczne pozyskiwanie danych\n",
    "\n",
    "## Tomasz Rodak\n",
    "\n",
    "Wykład 7\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c208d",
   "metadata": {},
   "source": [
    "## Scrapy\n",
    "\n",
    "Na podstawie: [Scrapy tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)\n",
    "\n",
    "Scrapy to framework do tworzenia botów sieciowych wykonujących ekstrakcję danych z witryn internetowych. Jest napisany w Pythonie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28aa5ac",
   "metadata": {},
   "source": [
    "### Tworzenie projektu\n",
    "\n",
    "Projekt to katalog, w którym znajdują się wszystkie pliki konfiguracyjne i kod źródłowy bota. Projekt tworzy się automatycznie poleceniem w terminalu:\n",
    "\n",
    "```python\n",
    "scrapy startproject <nazwa_projektu>\n",
    "```\n",
    "\n",
    "Przejdź do katalogu, w którym chcesz utworzyć projekt i uruchom polecenie: \n",
    "\n",
    "```bash\n",
    "scrapy startproject tutorial\n",
    "```\n",
    "\n",
    "Powstanie wówczas katalog `tutorial` postaci:\n",
    "\n",
    "```\n",
    "tutorial/\n",
    "    scrapy.cfg\n",
    "    tutorial/\n",
    "        __init__.py\n",
    "        items.py\n",
    "        middlewares.py\n",
    "        pipelines.py\n",
    "        settings.py\n",
    "        spiders/\n",
    "            __init__.py\n",
    "```\n",
    "\n",
    "- `scrapy.cfg` – główny plik konfiguracyjny projektu\n",
    "- `tutorial/` – katalog z kodem źródłowym bota\n",
    "    - `__init__.py` – inicjalizuje moduł\n",
    "    - `items.py` – definicje struktur danych (elementów zbieranych przez bota)\n",
    "    - `middlewares.py` – definicje middleware (pośredników obsługujących żądania i odpowiedzi)\n",
    "    - `pipelines.py` – definicje potoków przetwarzania danych\n",
    "    - `settings.py` – ustawienia projektu (np. limity, nagłówki, user-agent)\n",
    "    - `spiders/` – katalog z definicjami pająków (klas zbierających dane z witryn)\n",
    "        - `__init__.py` – inicjalizuje moduł spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61896070",
   "metadata": {},
   "source": [
    "### Pierwszy pająk\n",
    "\n",
    "Pająki to klasy dziedziczące po klasie `Spider`, które wykonują ekstrakcję danych z jednej lub wielu witryn internetowych. Muszą one definiować początkowe żądania do wykonania, a opcjonalnie także sposób podążania za linkami i przetwarzania pobranych stron w celu ekstrakcji danych.\n",
    "\n",
    "Oto kod naszego pierwszego pająka. Jego zadaniem będzie pobieranie stron z cytatami z witryny [quotes.toscrape.com](https://quotes.toscrape.com).\n",
    "Kod zapisz w pliku `quotes_spider.py` w katalogu `tutorial/spiders`:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    async def start(self):\n",
    "        urls = [\n",
    "            \"https://quotes.toscrape.com/page/1/\",\n",
    "            \"https://quotes.toscrape.com/page/2/\",\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")\n",
    "```\n",
    "\n",
    "Atrybuty i metody klasy `QuotesSpider`:\n",
    "- `name` – unikalna nazwa pająka, używana do identyfikacji go w projekcie.\n",
    "- `start()` – funkcja generatora zwracająca początkowe żądania (`Request`). Parametr `urls` to lista adresów URL, które pająk ma odwiedzić. Każde żądanie jest tworzone przez `scrapy.Request`, gdzie `url` to adres strony, a `callback` to metoda wywoływana po pobraniu strony.\n",
    "- `parse()` – metoda wywoływana po pobraniu strony. Odpowiada za przetwarzanie odpowiedzi i ekstrakcję danych. W tym przypadku zapisuje zawartość strony do pliku HTML. Parametr `response` jest instancją klasy `TextResponse`.\n",
    "\n",
    "Typowo metoda `parse()` odpowiada za przetwarzanie odpowiedzi z serwera – polega to na ekstrakcji interesujących danych (np. do słowników lub obiektów Item) oraz wyszukiwaniu nowych adresów URL do dalszego przetwarzania. Wykryte linki są następnie zwracane jako nowe żądania (`Request`), co pozwala pająkowi na rekurencyjne przeszukiwanie witryny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79ed31",
   "metadata": {},
   "source": [
    "### Uruchamianie pająka\n",
    "\n",
    "Aby uruchomić pająka, przejdź do katalogu projektu `tutorial` i użyj polecenia:\n",
    "\n",
    "```bash\n",
    "scrapy crawl quotes\n",
    "```\n",
    "\n",
    "Spowoduje to uruchomienie pająka `quotes`. \n",
    "\n",
    "Zrzut terminala:\n",
    "\n",
    "```\n",
    "...\n",
    "2025-06-01 10:45:25 [scrapy.core.engine] INFO: Spider opened\n",
    "2025-06-01 10:45:25 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2025-06-01 10:45:25 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2025-06-01 10:45:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
    "2025-06-01 10:45:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2025-06-01 10:45:26 [quotes] DEBUG: Saved file quotes-1.html\n",
    "2025-06-01 10:45:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
    "2025-06-01 10:45:26 [quotes] DEBUG: Saved file quotes-2.html\n",
    "2025-06-01 10:45:26 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "...\n",
    "```\n",
    "\n",
    "W katalogu projektu powinny pojawić się pliki `quotes-1.html` i `quotes-2.html`, zawierające pobrane strony z witryny."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5772a",
   "metadata": {},
   "source": [
    "### Skrót metody `start()`\n",
    "\n",
    "Metodę `start()` z kodu powyżej można uprościć zastępując ją przez atrybut klasy `start_urls`. Atrybut ten jest listą startowych adresów URL do odwiedzenia. Lista `start_urls` jest następnie wykorzystywana do automatycznej implementacji metody `start()`.\n",
    "\n",
    "Kod pająka po zmianie:\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"quotes-{page}.html\"\n",
    "        Path(filename).write_bytes(response.body)\n",
    "```\n",
    "\n",
    "Metoda `parse()` pozostaje bez zmian. Scrapy automatycznie wykorzystuje atrybut `start_urls` i wywołuje metodę `parse()` dla każdego adresu URL z tej listy. Framework oczekuje, że metoda przetwarzająca odpowiedzi będzie miała nazwę `parse`, a lista początkowych adresów – `start_urls`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba66134",
   "metadata": {},
   "source": [
    "### Ekstrakcja danych\n",
    "\n",
    "Scrapy dostarcza powłokę [Scrapy Shell](https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell) umożliwiającą interaktywne testowanie ekstrakcji danych z witryn. \n",
    "\n",
    "Uruchomienie powłoki Scrapy Shell w terminalu:\n",
    "\n",
    "```bash\n",
    "scrapy shell \"https://quotes.toscrape.com/page/1/\"\n",
    "```\n",
    "\n",
    "Obiekty dostępne w powłoce:\n",
    "\n",
    "```\n",
    "In [1]: shelp()\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   Out        {}\n",
    "[s]   _oh        {}\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x7f0a67ec7ec0>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://quotes.toscrape.com/page/1/>\n",
    "[s]   response   <200 https://quotes.toscrape.com/page/1/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x7f0a66e0c740>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bae64",
   "metadata": {},
   "source": [
    "W powłoce można wykonać selekcję danych z odpowiedzi HTTP selektorem XPath lub CSS:\n",
    "\n",
    "```python\n",
    "In [3]: response.xpath(\"//title\")\n",
    "Out[3]: [<Selector query='//title' data='<title>Quotes to Scrape</title>'>]\n",
    "In [4]: response.css(\"title\")\n",
    "Out[4]: [<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n",
    "```\n",
    "\n",
    "Element w postaci tekstowej:\n",
    "\n",
    "```python\n",
    "In [9]: response.xpath(\"//title\").get()\n",
    "Out[9]: '<title>Quotes to Scrape</title>'\n",
    "In [10]: response.xpath(\"//title/text()\").get()\n",
    "Out[10]: 'Quotes to Scrape'\n",
    "```\n",
    "\n",
    "Wszystkie elementy `small` z atrybutem `class=\"author\"`:\n",
    "\n",
    "```python\n",
    "In [16]: response.xpath(\"//small[attribute::class='author']\").getall()\n",
    "Out[16]: \n",
    "['<small class=\"author\" itemprop=\"author\">Albert Einstein</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">J.K. Rowling</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Albert Einstein</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Jane Austen</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Marilyn Monroe</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Albert Einstein</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">André Gide</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Thomas A. Edison</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Eleanor Roosevelt</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Steve Martin</small>']\n",
    "```\n",
    "\n",
    "Wartości tekstowe wszystkich elementów `small` z atrybutem `class=\"author\"`:\n",
    "\n",
    "```python\n",
    "In [18]: response.xpath(\"//small[attribute::class='author']/text()\").getall()\n",
    "Out[18]: \n",
    "['Albert Einstein',\n",
    " 'J.K. Rowling',\n",
    " 'Albert Einstein',\n",
    " 'Jane Austen',\n",
    " 'Marilyn Monroe',\n",
    " 'Albert Einstein',\n",
    " 'André Gide',\n",
    " 'Thomas A. Edison',\n",
    " 'Eleanor Roosevelt',\n",
    " 'Steve Martin']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1162fd",
   "metadata": {},
   "source": [
    "### Krótkie wprowadzenie do XPath\n",
    "\n",
    "[Concise XPath](http://plasmasturm.org/log/xpath101/)\n",
    "\n",
    "[XPath Tutorial](https://zvon.org/comp/r/tut-XPath_1.html#Pages~List_of_XPaths)\n",
    "\n",
    "[XPath Language Reference](https://www.w3.org/TR/xpath-31/)\n",
    "\n",
    "XPath to język zapytań służący do nawigowania i wybierania węzłów w dokumentach XML i HTML. Pozwala precyzyjnie określić, które elementy drzewa dokumentu mają zostać wybrane, korzystając z tzw. kroków, osi i predykatów.\n",
    "\n",
    "#### Podstawowe zasady działania XPath:\n",
    "- Każde wyrażenie XPath operuje na zbiorze węzłów (najczęściej zaczynając od korzenia dokumentu lub bieżącego węzła)\n",
    "- Wyrażenie składa się z sekwencji kroków oddzielonych ukośnikiem `/`, gdzie każdy krok wybiera nowy zbiór węzłów na podstawie określonych kryteriów\n",
    "- Kroki mogą być uzupełnione o predykaty (`[]`), które filtrują wybrane węzły\n",
    "- Oś (np. `child::`, `descendant::`, `following-sibling::`) określa relację nawigacji względem bieżącego węzła\n",
    "\n",
    "#### Kluczowe elementy składni:\n",
    "\n",
    "1. **Ukośnik `/`** – oddziela kolejne kroki nawigacji w drzewie dokumentu\n",
    "   ```\n",
    "   /foo/bar\n",
    "   - Zacznij od korzenia dokumentu\n",
    "   - Przejdź do elementów 'foo' będących bezpośrednimi dziećmi korzenia\n",
    "   - Następnie wybierz wszystkie elementy 'bar' będące dziećmi każdego 'foo'\n",
    "   ```\n",
    "   Ukośnik na początku (`/`) oznacza rozpoczęcie od korzenia dokumentu. Każdy kolejny ukośnik oddziela kolejne kroki (nie warunki!).\n",
    "\n",
    "2. **Krok** – pojedynczy etap nawigacji, np. `foo`, `@id`, `descendant::bar`. Każdy krok może zawierać predykaty:\n",
    "   ```\n",
    "   /foo[bar]\n",
    "   - Wybierz elementy 'foo' mające dziecko 'bar'\n",
    "   ```\n",
    "   Przejście do kolejnego kroku nie zawsze oznacza zejście głębiej w drzewie – zależy to od użytej osi.\n",
    "\n",
    "3. **Predykaty `[]`** – filtrują wybrane węzły na podstawie warunków:\n",
    "   ```\n",
    "   //div[@class='quote']\n",
    "   - Wybierz wszystkie elementy 'div' z atrybutem class=\"quote\"\n",
    "   ```\n",
    "   Predykaty mogą zawierać własne kroki i warunki.\n",
    "\n",
    "4. **Oś** – określa relację nawigacji względem bieżącego węzła. Najczęściej używana jest domyślna oś `child::`, ale można stosować inne, np.:\n",
    "   ```\n",
    "   /foo/following-sibling::bar\n",
    "   - Dla każdego 'foo' wybierz wszystkie elementy 'bar' będące jego rodzeństwem po prawej stronie\n",
    "   ```\n",
    "   Osi można używać zarówno jako kroku, jak i wewnątrz predykatów.\n",
    "\n",
    "5. **Przydatne skróty**:\n",
    "   - `@nazwa` zamiast `attribute::nazwa` (wybór atrybutu)\n",
    "   - `//foo` zamiast `/descendant-or-self::foo` (wszystkie elementy 'foo' w dokumencie)\n",
    "   - Domyślną osią jest `child::`, więc nie trzeba jej pisać jawnie\n",
    "\n",
    "W XPath można łączyć kroki, osie i predykaty, tworząc złożone i precyzyjne zapytania. Każdy predykat może zawierać kolejne kroki i predykaty, co pozwala na bardzo szczegółowe filtrowanie elementów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc830f8",
   "metadata": {},
   "source": [
    "### Ekstrakcja cytatów, autorów i tagów\n",
    "\n",
    "Korzystając z narzędzi deweloperskich przeglądarki widzimy, że każdy cytat w źródle strony ma postać:\n",
    "\n",
    "```html\n",
    "<div class=\"quote\" itemscope=\"\" itemtype=\"http://schema.org/CreativeWork\">\n",
    "        <span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>\n",
    "        <span>by <small class=\"author\" itemprop=\"author\">Albert Einstein</small>\n",
    "        <a href=\"/author/Albert-Einstein\">(about)</a>\n",
    "        </span>\n",
    "        <div class=\"tags\">\n",
    "            Tags:\n",
    "            <meta class=\"keywords\" itemprop=\"keywords\" content=\"change,deep-thoughts,thinking,world\"> \n",
    "            <a class=\"tag\" href=\"/tag/change/page/1/\">change</a>\n",
    "            <a class=\"tag\" href=\"/tag/deep-thoughts/page/1/\">deep-thoughts</a>\n",
    "            <a class=\"tag\" href=\"/tag/thinking/page/1/\">thinking</a>\n",
    "            <a class=\"tag\" href=\"/tag/world/page/1/\">world</a>  \n",
    "        </div>\n",
    "    </div>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472b4c8",
   "metadata": {},
   "source": [
    "W terminalu uruchamiamy powłokę Scrapy Shell:\n",
    "\n",
    "```bash\n",
    "scrapy shell \"https://quotes.toscrape.com/\n",
    "```\n",
    "\n",
    "Wyodrębniamy pierwszy cytat:\n",
    "\n",
    "```python\n",
    "In [1]: quote = response.xpath(\"//div[attribute::class='quote']\")[0]\n",
    "In [2]: quote\n",
    "Out[1]: <Selector query=\"//div[attribute::class='quote']\" data='<div class=\"quote\" itemscope itemtype...'>\n",
    "```\n",
    "\n",
    "Wyciągamy tekst cytatu:\n",
    "\n",
    "```python\n",
    "In [3]: quote.xpath(\"span[attribute::class='text']/text()\").get()\n",
    "Out[3]: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
    "```\n",
    "\n",
    "Autora:\n",
    "\n",
    "```python\n",
    "In [4]: quote.xpath(\"//small[attribute::class='author']/text()\").get()\n",
    "Out[4]: 'Albert Einstein'\n",
    "```\n",
    "\n",
    "Tagi:\n",
    "\n",
    "```python\n",
    "In [5]: quote.xpath(\"div[attribute::class='tags']/a/text()\").getall()\n",
    "Out[5]: \n",
    "['change', 'deep-thoughts', 'thinking', 'world']\n",
    "```\n",
    "\n",
    "To samo w wersji uproszczonej:\n",
    "\n",
    "```python\n",
    "In [6]: quote = response.xpath(\"//div[@class='quote']\")[0]\n",
    "In [7]: quote.xpath(\"span[@class='text']/text()\").get()\n",
    "Out[7]: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
    "In [8]: quote.xpath(\"span[@class='author']/text()\").get()\n",
    "Out[8]: 'Albert Einstein'\n",
    "In [9]: quote.xpath(\"div[@class='tags']/a/text()\").getall()\n",
    "Out[9]: \n",
    "['change', 'deep-thoughts', 'thinking', 'world']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2ab4b",
   "metadata": {},
   "source": [
    "### Ekstrakcja danych w pająku\n",
    "\n",
    "Metodę `parse()` piszemy w postaci funkcji generatora zwracającej słowniki z danymi:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\",\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.xpath(\"//div[@class='quote']\"):\n",
    "            yield {\n",
    "                \"text\": quote.xpath(\"span[@class='text']/text()\").get(),\n",
    "                \"author\": quote.xpath(\"span/small[@class='author']/text()\").get(),\n",
    "                \"tags\": quote.xpath(\"div[@class='tags']/a/text()\").getall(),\n",
    "            }\n",
    "```\n",
    "\n",
    "Uruchomienie pająka:\n",
    "\n",
    "```bash\n",
    "scrapy crawl quotes\n",
    "```\n",
    "\n",
    "Zrzut terminala:\n",
    "\n",
    "```\n",
    "...\n",
    "2025-06-02 21:49:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
    "2025-06-02 21:49:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "2025-06-02 21:49:20 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c22de65",
   "metadata": {},
   "source": [
    "### Zapisywanie danych do pliku\n",
    "\n",
    "Najprostszym sposobem na zapisanie danych z pająka do pliku jest użycie podczas uruchamiania opcji:\n",
    "- `-o` - określa nazwę pliku, do którego mają być **dopisane** dane\n",
    "- `-O` - określa nazwę pliku, do którego mają być zapisane dane, **nadpisując** jego zawartość o ile taki plik już istnieje\n",
    "\n",
    "```bash\n",
    "scrapy crawl quotes -O quotes.json\n",
    "```\n",
    "\n",
    "Spowoduje to zapisanie danych w formacie JSON do pliku `quotes.json`. Można również użyć innych formatów, takich jak CSV lub XML, zmieniając rozszerzenie pliku:\n",
    "\n",
    "```bash\n",
    "scrapy crawl quotes -O quotes.xml\n",
    "```\n",
    "\n",
    "Dopisywanie nowych wartości do istniejącego pliku JSON może prowadzić do niepoprawnego formatu. Z tego powodu lepiej jest korzystać z formatu JSON Lines (JSONL):\n",
    "\n",
    "```bash\n",
    "scrapy crawl quotes -o quotes.jsonl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855c0a4",
   "metadata": {},
   "source": [
    "### Podążanie za linkami\n",
    "\n",
    "Strona [quotes.toscrape.com](https://quotes.toscrape.com) zawiera na spodzie linki do kolejnych stron z cytatami w elementach postaci:\n",
    "\n",
    "```html\n",
    "<ul class=\"pager\">\n",
    "            <li class=\"next\">\n",
    "                <a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>\n",
    "            </li>         \n",
    "        </ul>\n",
    "```\n",
    "\n",
    "Wyodrębnienie w Scrapy Shell:\n",
    "\n",
    "```python\n",
    "In [5]: response.xpath(\"//ul[@class='pager']/li[@class='next']/a/@href\").get()\n",
    "Out[5]: '/page/2/'\n",
    "```\n",
    "\n",
    "Aby pająk mógł podążać za tymi linkami, należy zmodyfikować metodę `parse()` tak, aby rekurencyjnie generowała nowe żądania dla kolejnych stron:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.xpath(\"//div[@class='quote']\"):\n",
    "            yield {\n",
    "                \"text\": quote.xpath(\"span[@class='text']/text()\").get(),\n",
    "                \"author\": quote.xpath(\"span/small[@class='author']/text()\").get(),\n",
    "                \"tags\": quote.xpath(\"div[@class='tags']/a/text()\").getall(),\n",
    "            }\n",
    "        \n",
    "        next_page = response.xpath(\"//ul[@class='pager']/li[@class='next']/a/@href\").get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)  # Tworzy pełny URL\n",
    "            self.log(f\"Przechodzę do następnej strony: {next_page}\")\n",
    "            yield scrapy.Request(url=next_page, callback=self.parse)\n",
    "```\n",
    "\n",
    "Pierwsze wywołanie iteratora zwróci dane z pierwszej strony. Wywołanie kolejne spowoduje wstawienie żądania `Request` do harmonogramu przetwarzania. `callback` tego żądania to ponownie metoda `parse()`, co pozwala na rekurencyjne przetwarzanie kolejnych stron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9677a78",
   "metadata": {},
   "source": [
    "### Podążanie za linkami\n",
    "\n",
    "Strona [quotes.toscrape.com](https://quotes.toscrape.com) zawiera na dole każdej strony linki nawigacyjne umożliwiające przejście do kolejnych stron z cytatami:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec08d80d",
   "metadata": {},
   "source": [
    "```html\n",
    "<ul class=\"pager\">\n",
    "            <li class=\"next\">\n",
    "                <a href=\"/page/2/\">Next <span aria-hidden=\"true\">→</span></a>\n",
    "            </li>         \n",
    "        </ul>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a65433",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Wyodrębnienie odnośnika do następnej strony w Scrapy Shell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a8ced",
   "metadata": {
    "vscode": {
     "languageId": "code-referencing"
    }
   },
   "source": [
    "```\n",
    "In [5]: response.xpath(\"//ul[@class='pager']/li[@class='next']/a/@href\").get()\n",
    "Out[5]: '/page/2/'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa6d37",
   "metadata": {},
   "source": [
    "Aby pająk mógł przetwarzać kolejne strony, należy zmodyfikować metodę `parse()` tak, aby oprócz ekstrakcji danych generowała również żądania do kolejnych stron:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef75b80",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.xpath(\"//div[@class='quote']\"):\n",
    "            yield {\n",
    "                \"text\": quote.xpath(\"span[@class='text']/text()\").get(),\n",
    "                \"author\": quote.xpath(\"span/small[@class='author']/text()\").get(),\n",
    "                \"tags\": quote.xpath(\"div[@class='tags']/a/text()\").getall(),\n",
    "            }\n",
    "        \n",
    "        next_page = response.xpath(\"//ul[@class='pager']/li[@class='next']/a/@href\").get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)  # Tworzy pełny URL\n",
    "            yield scrapy.Request(url=next_page, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da38118",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "W tym kodzie występują dwa rodzaje wyrażeń `yield`:\n",
    "1. Pierwsze generuje słowniki z danymi zebranymi z aktualnej strony (cytatami)\n",
    "2. Drugie generuje nowe żądanie HTTP do kolejnej strony (o ile istnieje)\n",
    "\n",
    "Mechanizm działa rekurencyjnie - dla każdej nowej strony ponownie wywoływana jest ta sama metoda `parse()`, która ponownie ekstrahuje dane i szuka linku do następnej strony. Proces trwa aż do napotkania ostatniej strony, która nie zawiera odnośnika \"Next\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4137a0d6",
   "metadata": {},
   "source": [
    "### Skrót do żądania Request\n",
    "\n",
    "W powyższym kodzie używamy metody `response.urljoin(next_page)`, aby utworzyć pełny URL do następnej strony. Można to uprościć, korzystając z funkcji `response.follow()`:\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe527b",
   "metadata": {},
   "source": [
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.xpath(\"//div[@class='quote']\"):\n",
    "            yield {\n",
    "                \"text\": quote.xpath(\"span[@class='text']/text()\").get(),\n",
    "                \"author\": quote.xpath(\"span/small[@class='author']/text()\").get(),\n",
    "                \"tags\": quote.xpath(\"div[@class='tags']/a/text()\").getall(),\n",
    "            }\n",
    "        \n",
    "        next_page = response.xpath(\"//ul[@class='pager']/li[@class='next']/a/@href\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b017b4",
   "metadata": {},
   "source": [
    "### Zbieranie danych o autorach\n",
    "\n",
    "Pająk z dwiema metodami parsującymi:\n",
    "- `parse()` – zbiera cytaty ze strony oraz wyszukuje linki do stron autorów\n",
    "- `parse_author()` – odwiedza stronę każdego autora i zbiera szczegółowe dane o nim\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = \"author\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Znajdź wszystkie linki do stron autorów na bieżącej stronie\n",
    "        author_links = response.xpath(\"//small[@class='author']/following-sibling::a/@href\")\n",
    "        # Dla każdego linku do autora utwórz żądanie i przekaż do metody parse_author\n",
    "        yield from response.follow_all(author_links, callback=self.parse_author)\n",
    "\n",
    "        # Sprawdź, czy istnieje link do następnej strony z cytatami\n",
    "        next_page = response.xpath(\"//ul[@class='pager']/li[@class='next']/a/@href\").get()\n",
    "        if next_page is not None:\n",
    "            # Jeśli tak, przejdź do kolejnej strony i powtórz proces\n",
    "            yield response.follow(next_page, callback=self.parse)\n",
    "\n",
    "    def parse_author(self, response):\n",
    "        # Wyodrębnij dane o autorze ze strony autora\n",
    "        yield {\n",
    "            \"author\": response.xpath(\"//h3[@class='author-title']/text()\").get(),\n",
    "            \"born\": response.xpath(\"//span[@class='author-born-date']/text()\").get()\n",
    "        }\n",
    "```\n",
    "\n",
    "W powyższym kodzie:\n",
    "- W metodzie `parse()` wyszukiwane są wszystkie linki prowadzące do stron autorów na bieżącej stronie z cytatami. Funkcja `yield from response.follow_all(author_links, callback=self.parse_author)` automatycznie tworzy żądania HTTP do wszystkich znalezionych linków i przekazuje je do metody `parse_author`, która przetwarza odpowiedzi z tych stron.\n",
    "- Następnie, jeśli istnieje link do kolejnej strony z cytatami, pająk przechodzi na tę stronę i ponownie wykonuje ekstrakcję linków do autorów oraz ewentualnie przechodzi dalej.\n",
    "- Metoda `parse_author()` jest wywoływana dla każdej strony autora i wyodrębnia szczegółowe dane, takie jak imię i nazwisko autora oraz data urodzenia.\n",
    "\n",
    "Składnia:\n",
    "\n",
    "```\n",
    "yield from response.follow_all(author_links, callback=self.parse_author)\n",
    "```\n",
    "\n",
    "jest skrótem do iteracji po wszystkich znalezionych linkach i generowania żądań `Request` dla każdego z nich. Każda odpowiedź z tych żądań trafia do metody `parse_author`, gdzie następuje ekstrakcja danych o autorze."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
